# This pipeline configures Logstash to:
# 1. Listen for logs from Docker's gelf driver.
# 2. Parse the 'message' field (which contains our app's JSON log) as JSON.
# 3. Send the result to Elasticsearch.

input {
  gelf {
    # This listens on port 12201/udp, matching the docker-compose.yml config
    # The 'gelf' input automatically parses the gelf-formatted message.
    # Our app's JSON string will be in the 'message' field.
  }
}

filter {
  # The 'message' field now contains the *stringified* JSON from our app.
  # This filter parses that string into a structured JSON object.
  json {
    source => "message"
    # 'target => "doc"' could be used to place it in a sub-field
    # By default, it parses it into the root of the logstash event.
  }

  # Clean up fields we don't need
  mutate {
    remove_field => ["message", "host", "_level", "_facility", "_timestamp"]
  }
}

output {
  # Send the processed logs to our Elasticsearch service
  elasticsearch {
    hosts => ["http://elasticsearch:9200"] # 'elasticsearch' is the service name
    index => "logstash-fastapi-%{+YYYY.MM.dd}"
  }

  # (Optional) Uncomment this for debugging in your console
  # stdout {
  #   codec => rubydebug
  # }
}